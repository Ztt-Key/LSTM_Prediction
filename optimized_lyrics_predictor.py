#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆæ­Œè¯é¢„æµ‹å™¨
è§£å†³é¢„æµ‹ç»“æœé‡å¤å’Œè´¨é‡é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import jieba
import numpy as np
import json
import os
import pickle
import random
from collections import Counter, defaultdict
from torch.utils.data import Dataset, DataLoader
import re
from tqdm import tqdm


class OptimizedLyricsDataset(Dataset):
    """ä¼˜åŒ–çš„æ­Œè¯æ•°æ®é›†"""
    def __init__(self, sequences, vocab_to_idx, seq_length=12):
        self.vocab_to_idx = vocab_to_idx
        self.seq_length = seq_length
        
        # è¿‡æ»¤å’Œæ ‡å‡†åŒ–åºåˆ—
        self.sequences = []
        for seq in sequences:
            if len(seq) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªè¯ï¼ˆè¾“å…¥å’Œç›®æ ‡ï¼‰
                # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæˆªæ–­ï¼›å¦‚æœå¤ªçŸ­ï¼Œä¿æŒåŸæ ·
                if len(seq) > seq_length + 1:
                    normalized_seq = seq[:seq_length + 1]
                else:
                    normalized_seq = seq
                self.sequences.append(normalized_seq)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        input_seq = sequence[:-1]
        target_seq = sequence[1:]
        
        # è½¬æ¢ä¸ºç´¢å¼•
        input_indices = [self.vocab_to_idx.get(word, self.vocab_to_idx['<UNK>']) for word in input_seq]
        target_indices = [self.vocab_to_idx.get(word, self.vocab_to_idx['<UNK>']) for word in target_seq]
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(input_indices) < self.seq_length:
            input_indices.append(self.vocab_to_idx['<PAD>'])
        while len(target_indices) < self.seq_length:
            target_indices.append(self.vocab_to_idx['<PAD>'])
        
        # æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        input_indices = input_indices[:self.seq_length]
        target_indices = target_indices[:self.seq_length]
        
        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)


class OptimizedLyricsLSTM(nn.Module):
    """ä¼˜åŒ–çš„LSTMæ¨¡å‹"""
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):
        super(OptimizedLyricsLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                           batch_first=True, dropout=dropout, bidirectional=False)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
        # æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dropout=dropout)
        
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        lstm_out, hidden = self.lstm(embedded, hidden)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        lstm_out_transposed = lstm_out.transpose(0, 1)  # (seq_len, batch, hidden_dim)
        attn_out, _ = self.attention(lstm_out_transposed, lstm_out_transposed, lstm_out_transposed)
        attn_out = attn_out.transpose(0, 1)  # (batch, seq_len, hidden_dim)
        
        # ç»“åˆLSTMè¾“å‡ºå’Œæ³¨æ„åŠ›è¾“å‡º
        combined = lstm_out + attn_out
        combined = self.dropout(combined)
        output = self.fc(combined)
        
        return output, hidden


class OptimizedLyricsPredictor:
    """ä¼˜åŒ–ç‰ˆæ­Œè¯é¢„æµ‹å™¨"""
    
    def __init__(self, model_path='optimized_lyrics_model.pth', vocab_path='optimized_vocab.pkl'):
        self.model_path = model_path
        self.vocab_path = vocab_path
        self.model = None
        self.vocab_to_idx = {}
        self.idx_to_vocab = {}
        self.seq_length = 12
        
        # é¢„å®šä¹‰çš„é«˜è´¨é‡æ­Œè¯æ¨¡æ¿
        self.quality_templates = [
            "{emotion}çš„{time}é‡Œï¼Œ{action}{object}",
            "{place}çš„{object}{verb}ï¼Œ{feeling}åœ¨å¿ƒä¸­",
            "{adjective}çš„{noun}å¦‚{metaphor}ï¼Œ{description}",
            "å½“{condition}çš„æ—¶å€™ï¼Œ{result}",
            "{subject}{verb}{object}ï¼Œ{emotion}{feeling}",
        ]
        
        # è¯æ±‡åˆ†ç±»å­—å…¸
        self.word_categories = {
            'emotion': ['æ¸©æš–', 'ç”œç¾', 'æµªæ¼«', 'æ·±æƒ…', 'çº¯çœŸ', 'ç¾å¥½'],
            'time': ['æ˜¥å¤©', 'å¤æ—¥', 'ç§‹å¤©', 'å†¬å­£', 'æ¸…æ™¨', 'é»„æ˜', 'å¤œæ™š'],
            'place': ['å±±é—´', 'æµ·è¾¹', 'æ—ä¸­', 'èŠ±å›­', 'æ¡¥è¾¹', 'æ¹–ç•”', 'è‰åŸ'],
            'object': ['èŠ±æœµ', 'æœˆäº®', 'æ˜Ÿç©º', 'æ²³æµ', 'å¾®é£', 'é˜³å…‰', 'å½©è™¹'],
            'verb': ['ç››å¼€', 'ç…§è€€', 'æµæ·Œ', 'é£ç¿”', 'æ­Œå”±', 'èˆè¹ˆ', 'é—ªçƒ'],
            'feeling': ['æ€å¿µ', 'æ¸©æš–', 'å¹¸ç¦', 'å¿«ä¹', 'æ„ŸåŠ¨', 'å®é™', 'å–œæ‚¦'],
            'adjective': ['ç¾ä¸½', 'æ¸…æ–°', 'åŠ¨äºº', 'è¿·äºº', 'ç»šä¸½', 'å¯çˆ±'],
            'noun': ['æ¢¦æƒ³', 'å¸Œæœ›', 'å‹è°Š', 'çˆ±æƒ…', 'é’æ˜¥', 'å›å¿†', 'æœªæ¥'],
            'metaphor': ['æµæ°´', 'æ˜¥é£', 'æœéœ', 'æ˜Ÿè¾°', 'èŠ±é¦™', 'éŸ³ç¬¦'],
        }
    
    def get_enhanced_lyrics_data(self):
        """è·å–å¢å¼ºçš„æ­Œè¯æ•°æ®"""
        lyrics_data = [
            # ç»å…¸è¯—è¯é£æ ¼
            "æ˜¥æ±ŸèŠ±æœˆå¤œï¼Œæ˜æœˆå‡ æ—¶æœ‰",
            "å±±é‡æ°´å¤ç–‘æ— è·¯ï¼ŒæŸ³æš—èŠ±æ˜åˆä¸€æ‘", 
            "æµ·å†…å­˜çŸ¥å·±ï¼Œå¤©æ¶¯è‹¥æ¯”é‚»",
            "è½çº¢ä¸æ˜¯æ— æƒ…ç‰©ï¼ŒåŒ–ä½œæ˜¥æ³¥æ›´æŠ¤èŠ±",
            "é‡‡èŠä¸œç¯±ä¸‹ï¼Œæ‚ ç„¶è§å—å±±",
            "ä¼šå½“å‡Œç»é¡¶ï¼Œä¸€è§ˆä¼—å±±å°",
            "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœ",
            "æ˜¥çœ ä¸è§‰æ™“ï¼Œå¤„å¤„é—»å•¼é¸Ÿ",
            "ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµ",
            "ç‹¬åœ¨å¼‚ä¹¡ä¸ºå¼‚å®¢ï¼Œæ¯é€¢ä½³èŠ‚å€æ€äº²",
            
            # ç°ä»£æµè¡Œé£æ ¼
            "ä½ æ˜¯æˆ‘å¿ƒä¸­æœ€ç¾çš„é£æ™¯",
            "æ—¶é—´ä¼šå¸¦èµ°ä¸€åˆ‡çƒ¦æ¼",
            "æ¯ä¸€ä¸ªæ˜å¤©éƒ½å……æ»¡å¸Œæœ›",
            "æ¢¦æƒ³çš„ç¿…è†€å¸¦æˆ‘é£ç¿”",
            "å¾®ç¬‘æ˜¯æœ€ç¾çš„è¯­è¨€",
            "ç›¸é‡æ˜¯ç¾ä¸½çš„ç¼˜åˆ†",
            "å²æœˆå¦‚æ­Œå£°å£°å…¥è€³",
            "é’æ˜¥ä¸æ•£åœºæ¢¦æƒ³ä¸æ‰“çƒŠ",
            "æ˜Ÿå…‰ç‚¹ç‚¹ç…§äº®å¤œç©º",
            "é£é›¨è¿‡åè§å½©è™¹",
            "çˆ±å¦‚æ˜¥é£æš–äººå¿ƒ",
            "æƒ…å¦‚ç»†é›¨æ¶¦å¿ƒç”°",
            "å‹‡æ•¢é¢å¯¹æ¯ä¸€å¤©",
            "å¿ƒä¸­æœ‰çˆ±ä¸å­¤å•",
            "çœ¼ä¸­æœ‰å…‰ä¸è¿·èŒ«",
            "å‹è°Šå¦‚èŠ±æ°¸è¿œå¼€",
            "çœŸæƒ…å¦‚æ°´æ°¸è¿œæµ",
            "æ—¶å…‰é£é€å¦‚æµæ°´",
            "å²æœˆå¦‚æ­Œæ°¸æµä¼ ",
            
            # æƒ…æ„ŸæŠ’å‘
            "æ€å›ä¸è§å›æƒ³å›å›ä¸çŸ¥",
            "ç›¸æ€å¦‚æ½®æ°´æ€å¿µå¦‚æµäº‘",
            "åƒé‡Œå…±å©µå¨Ÿä½†æ„¿äººé•¿ä¹…",
            "æ˜¥èŠ±ç§‹æœˆä½•æ—¶äº†",
            "æœ€ç¾ä¸è¿‡åˆç›¸è§",
            "æƒ…æ·±æ·±é›¨è’™è’™",
            "çº¢è±†ç”Ÿå—å›½æ˜¥æ¥å‘å‡ æ",
            "æ„¿å¾—ä¸€å¿ƒäººç™½å¤´ä¸ç›¸ç¦»",
            "ç¦»åˆ«æ€»æ˜¯åœ¨ä¹æœˆ",
            "å›å¿†æ˜¯æ€å¿µçš„æ„",
            "æœ€è‹¦ä¸è¿‡ç¦»åˆ«æ—¶",
            "å¤šå°‘æ¥¼å°çƒŸé›¨ä¸­",
            "å±±æ— é™µæ±Ÿæ°´ä¸ºç«­",
            "å†¬é›·éœ‡éœ‡å¤é›¨é›ª",
            
            # åŠ±å¿—æ­£èƒ½é‡
            "ç›¸ä¿¡è‡ªå·±ä¸€å®šèƒ½è¡Œ",
            "å›°éš¾é¢å‰ä¸ä½å¤´",
            "æ¯ä¸€æ¬¡è·Œå€’éƒ½æ˜¯æˆé•¿",
            "å‹‡æ•¢è¿½æ±‚å¿ƒä¸­çš„æ¢¦",
            "ä»Šå¤©çš„åŠªåŠ›æ˜¯æ˜å¤©çš„æ”¶è·",
            "æœºä¼šæ€»æ˜¯ç•™ç»™æœ‰å‡†å¤‡çš„äºº",
            "åªè¦å¿ƒä¸­æœ‰é˜³å…‰",
            "ç”¨å¾®ç¬‘é¢å¯¹ç”Ÿæ´»",
            "å¥‹æ–—çš„é’æ˜¥æœ€ç¾ä¸½",
            "åšæŒåˆ°åº•å°±æ˜¯èƒœåˆ©",
            "æŒ«æŠ˜é¢å‰ä¸æ°”é¦",
            "æ¯ä¸€æ¬¡å¤±è´¥éƒ½æ˜¯ç»éªŒ",
            "æ°¸è¿œä¸è¦è¯´æ”¾å¼ƒ",
            "ä»Šå¤©çš„æ±—æ°´æ˜¯æ˜å¤©çš„å½©è™¹",
            "æˆåŠŸæ€»æ˜¯å±äºåšæŒçš„äºº",
            "è·¯è™½è¿œè¡Œåˆ™å°†è‡³",
            "äº‹è™½éš¾åšåˆ™å¿…æˆ",
            "ç”Ÿæ´»å°±ä¼šæœ‰å¸Œæœ›",
            "ç”¨çœŸè¯šå¯¹å¾…æœ‹å‹",
            "æ‹¼æçš„äººç”Ÿæœ€ç²¾å½©",
            
            # è‡ªç„¶é£å…‰
            "æ˜¥æš–èŠ±å¼€ä¸‡ç‰©è‹",
            "ç§‹é«˜æ°”çˆ½ä¸¹æ¡‚é¦™",
            "å°æºªæ½ºæ½ºæµå‘æµ·",
            "é«˜å±±å·å³¨å…¥äº‘ç«¯",
            "æ£®æ—æ·±å¤„é¸Ÿè¯­èŠ±é¦™",
            "æµ·æµªæ‹å²¸å£°å£°ç¾",
            "æœéœæ»¡å¤©çº¢ä¼¼ç«",
            "æ˜Ÿè¾°å¤§æµ·ä»»é¨æ¸¸",
            "èŠ±å¼€èŠ±è½æœ‰æ—¶èŠ‚",
            "å±±æ¸…æ°´ç§€å¥½é£å…‰",
            "å¤æ—¥ç‚ç‚ç»¿è«æµ“",
            "å†¬é›ªçº·é£æ¢…èŠ±å¼€",
            "å¤§æ²³æ»”æ»”å¥”å‘å‰",
            "æ·±è°·å¹½é™è—ä»™å¢ƒ",
            "è‰åŸè¾½é˜”é©¬å„¿å¥”è·‘",
            "æµ·é¸¥ç¿±ç¿”å¤©åœ°é—´",
            "å¤•é˜³è¥¿ä¸‹é‡‘æ»¡å±±",
            "äº‘å·äº‘èˆ’è‡ªæ‚ ç„¶",
            "äº‘æ¥äº‘å»æ— å®šæ‰€",
            "é¸Ÿè¯­èŠ±é¦™é†‰äººå¿ƒ",
            
            # æ°‘æ—é£æ ¼
            "èŒ‰è‰èŠ±å¼€æ»¡å›­é¦™",
            "é‡‡èŠ±å§‘å¨˜å¿ƒæ¬¢ç•…",
            "å±±æ­Œå¥½æ¯”æ˜¥æ±Ÿæ°´",
            "ä¸æ€•æ»©é™©å¼¯åˆå¤š",
            "é˜¿é‡Œå±±çš„å§‘å¨˜ç¾å¦‚æ°´",
            "é˜¿é‡Œå±±çš„å°‘å¹´å£®å¦‚å±±",
            "è‰åŸä¸Šå‡èµ·ä¸è½çš„å¤ªé˜³",
            "ç…§è€€ç€æˆ‘ä»¬ç¾ä¸½çš„å®¶ä¹¡",
            "åŒ—é£é‚£ä¸ªå¹é›ªèŠ±é‚£ä¸ªé£˜",
            "æ±Ÿå—çƒŸé›¨è’™è’™",
            "å°æ¡¥æµæ°´äººå®¶",
            "é»„åœŸé«˜åŸå”±å±±æ­Œ",
            "ä¿¡å¤©æ¸¸å£°éœ‡å±±æ²³",
            "ç«¹æ—æ·±å¤„æœ‰äººå®¶",
            "ç¿ ç»¿ç«¹å¶æ˜ æœéœ",
            "æ¸”èˆŸå”±æ™šæ»¡æ±Ÿçº¢",
            "å¤•é˜³è¥¿ä¸‹æ°´æ³¢æ¸…",
            "æ¢…èŠ±æœµæœµå¼€æ»¡å±±",
            "é›ªèŠ±ç‰‡ç‰‡èˆæ»¡å¤©",
            "æ¡ƒèŠ±æºé‡Œå¥½é£å…‰",
            "ä¸–å¤–æ¡ƒæºåœ¨å¿ƒä¸­",
            "æ˜¥é£åˆç»¿æ±Ÿå—å²¸",
            "æ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜",
            "å¡å¤–é£å…‰æ— é™å¥½",
            "è‰åŸå„¿å¥³æƒ…æ„é•¿",
            
            # ç”Ÿæ´»æ„Ÿæ‚Ÿ
            "å¹³å‡¡çš„ç”Ÿæ´»ä¹Ÿæœ‰è¯—æ„",
            "ç®€å•çš„æ—¥å­ä¹Ÿæœ‰ç¾ä¸½",
            "å®¶æ˜¯å¿ƒçµçš„æ¸¯æ¹¾",
            "çˆ±æ˜¯ç”Ÿå‘½çš„æºæ³‰",
            "å¥åº·æ˜¯æœ€å¤§çš„è´¢å¯Œ",
            "å¿«ä¹æ˜¯æœ€å¥½çš„ç¤¼ç‰©",
            "çæƒœçœ¼å‰çš„å¹¸ç¦",
            "æ„Ÿæ©èº«è¾¹çš„æ¸©æš–",
            "ç”Ÿæ´»å¦‚èŒ¶éœ€è¦æ…¢æ…¢å“",
            "äººç”Ÿå¦‚ä¹¦éœ€è¦ç»†ç»†è¯»",
            "çŸ¥è¶³å¸¸ä¹å¿ƒè‡ªåœ¨",
            "åŠ©äººä¸ºä¹æƒ…è‡ªçœŸ",
            "åšäººå¦‚æ°´èƒ½è½½èˆŸ",
            "åšäº‹å¦‚å±±èƒ½æ‹…å½“",
            "çœŸè¯šå¾…äººäººå¾…çœŸ",
            "å–„è‰¯å¤„ä¸–ä¸–å¤„å–„",
            "å®½å®¹æ˜¯æ™ºæ…§çš„ä½“ç°",
            "ç†è§£æ˜¯å‹è°Šçš„åŸºç¡€",
            "ä»Šå¤©å¾ˆæ®‹é…·æ˜å¤©æ›´æ®‹é…·",
            "åå¤©å¾ˆç¾å¥½",
        ]
        
        # ç”Ÿæˆæ›´å¤šå˜ä½“
        extended_data = []
        for lyric in lyrics_data:
            extended_data.append(lyric)
            # ä¸ºæ¯ä¸ªæ­Œè¯ç”Ÿæˆ1-2ä¸ªå˜ä½“
            variations = self.generate_lyric_variations(lyric)
            extended_data.extend(variations[:2])
        
        return extended_data
    
    def generate_lyric_variations(self, original_lyric):
        """ç”Ÿæˆæ­Œè¯å˜ä½“"""
        variations = []
        words = list(jieba.cut(original_lyric))
        
        # åŒä¹‰è¯æ›¿æ¢
        synonyms = {
            'ç¾ä¸½': ['ç¾å¥½', 'æ¼‚äº®', 'åŠ¨äºº', 'è¿·äºº'],
            'æ¸©æš–': ['æ¸©é¦¨', 'æš–å’Œ', 'æ¸©æŸ”', 'äº²åˆ‡'],
            'å¿«ä¹': ['å¼€å¿ƒ', 'æ„‰å¿«', 'é«˜å…´', 'æ¬¢ä¹'],
            'æ¢¦æƒ³': ['ç†æƒ³', 'å¿ƒæ„¿', 'æ„¿æœ›', 'æ†§æ†¬'],
            'å¸Œæœ›': ['æœŸæœ›', 'ç›¼æœ›', 'æ¸´æœ›', 'ä¼ç›¼'],
            'çˆ±æƒ…': ['çœŸçˆ±', 'æ·±æƒ…', 'æ‹æƒ…', 'çˆ±æ„'],
        }
        
        # ç”Ÿæˆæ›¿æ¢å˜ä½“
        for i, word in enumerate(words):
            if word in synonyms:
                for synonym in synonyms[word][:2]:  # æœ€å¤š2ä¸ªåŒä¹‰è¯
                    new_words = words.copy()
                    new_words[i] = synonym
                    variation = ''.join(new_words)
                    if variation != original_lyric:
                        variations.append(variation)
        
        return variations
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰\s]', '', text)
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = list(jieba.cut(text.strip()))
        
        # è¿‡æ»¤å’Œæ¸…ç†
        cleaned_words = []
        for word in words:
            word = word.strip()
            if word and len(word) > 0:
                cleaned_words.append(word)
        
        return cleaned_words
    
    def build_optimized_vocab(self):
        """æ„å»ºä¼˜åŒ–çš„è¯æ±‡è¡¨"""
        print("æ­£åœ¨æ„å»ºä¼˜åŒ–è¯æ±‡è¡¨...")
        
        # è·å–æ­Œè¯æ•°æ®
        lyrics_data = self.get_enhanced_lyrics_data()
        print(f"æ”¶é›†åˆ° {len(lyrics_data)} æ¡æ­Œè¯æ•°æ®")
        
        # é¢„å¤„ç†æ‰€æœ‰æ­Œè¯
        all_words = []
        processed_lyrics = []
        
        for lyric in lyrics_data:
            words = self.preprocess_text(lyric)
            if len(words) >= 3:
                all_words.extend(words)
                processed_lyrics.append(words)
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(all_words)
        
        # æ„å»ºè¯æ±‡è¡¨
        vocab = ['<PAD>', '<UNK>', '<START>', '<END>']
        
        # æ·»åŠ é«˜é¢‘è¯ï¼ˆå‡ºç°æ¬¡æ•°>=2ï¼‰
        frequent_words = [word for word, count in word_counts.most_common() if count >= 2]
        vocab.extend(frequent_words)
        
        # æ·»åŠ é‡è¦çš„å•å­—è¯
        important_chars = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å´', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'æ›´', 'æœ€']
        for char in important_chars:
            if char not in vocab:
                vocab.append(char)
        
        self.vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx_to_vocab = {idx: word for word, idx in self.vocab_to_idx.items()}
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(self.vocab_path, 'wb') as f:
            pickle.dump((self.vocab_to_idx, self.idx_to_vocab), f)
        
        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå…±åŒ…å« {len(vocab)} ä¸ªè¯")
        return processed_lyrics
    
    def prepare_training_data(self, lyrics_list):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        sequences = []
        
        for lyric_words in lyrics_list:
            if len(lyric_words) < 2:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                continue
            
            # æ·»åŠ ç‰¹æ®Šæ ‡è®°
            words = ['<START>'] + lyric_words + ['<END>']
            
            # åˆ›å»ºå¤šç§é•¿åº¦çš„æ»‘åŠ¨çª—å£åºåˆ—
            max_len = min(len(words) - 1, self.seq_length)
            for seq_len in range(3, max_len + 1):  # ä»3åˆ°æœ€å¤§é•¿åº¦
                for i in range(len(words) - seq_len):
                    sequence = words[i:i + seq_len + 1]
                    sequences.append(sequence)
        
        return sequences
    
    def train_optimized_model(self, epochs=80, batch_size=16, learning_rate=0.001):
        """è®­ç»ƒä¼˜åŒ–æ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒä¼˜åŒ–æ­Œè¯é¢„æµ‹æ¨¡å‹...")
        
        # æ„å»ºè¯æ±‡è¡¨å’Œå‡†å¤‡æ•°æ®
        processed_lyrics = self.build_optimized_vocab()
        sequences = self.prepare_training_data(processed_lyrics)
        
        if not sequences:
            print("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
            return
        
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab_to_idx)}")
        print(f"è®­ç»ƒåºåˆ—æ•°é‡: {len(sequences)}")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = OptimizedLyricsDataset(sequences, self.vocab_to_idx, self.seq_length)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        vocab_size = len(self.vocab_to_idx)
        self.model = OptimizedLyricsLSTM(
            vocab_size=vocab_size,
            embedding_dim=128,
            hidden_dim=256,
            num_layers=2,
            dropout=0.3
        )
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss(ignore_index=self.vocab_to_idx['<PAD>'])
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.8)
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        best_loss = float('inf')
        
        for epoch in range(epochs):
            total_loss = 0
            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
            
            for batch_input, batch_target in progress_bar:
                optimizer.zero_grad()
                
                output, _ = self.model(batch_input)
                loss = criterion(output.reshape(-1, vocab_size), batch_target.reshape(-1))
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                progress_bar.set_postfix({'loss': loss.item()})
            
            scheduler.step()
            avg_loss = total_loss / len(dataloader)
            
            if avg_loss < best_loss:
                best_loss = avg_loss
                torch.save(self.model.state_dict(), self.model_path)
            
            if (epoch + 1) % 10 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.4f}')
        
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {self.model_path}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            with open(self.vocab_path, 'rb') as f:
                self.vocab_to_idx, self.idx_to_vocab = pickle.load(f)
            
            vocab_size = len(self.vocab_to_idx)
            self.model = OptimizedLyricsLSTM(vocab_size=vocab_size)
            self.model.load_state_dict(torch.load(self.model_path, map_location='cpu'))
            self.model.eval()
            
            print("ä¼˜åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
        except FileNotFoundError:
            print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return False
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def predict_with_quality_control(self, input_line, num_predictions=3):
        """å¸¦è´¨é‡æ§åˆ¶çš„é¢„æµ‹"""
        if self.model is None:
            return ["æ¨¡å‹æœªåŠ è½½"]
        
        try:
            predictions = []
            
            # æ–¹æ³•1: åŸºäºæ¨¡å‹çš„é¢„æµ‹
            model_predictions = self._model_based_prediction(input_line, num_predictions)
            predictions.extend(model_predictions)
            
            # æ–¹æ³•2: åŸºäºæ¨¡æ¿çš„é¢„æµ‹
            if len(predictions) < num_predictions:
                template_predictions = self._template_based_prediction(input_line, num_predictions - len(predictions))
                predictions.extend(template_predictions)
            
            # æ–¹æ³•3: åŸºäºè§„åˆ™çš„é¢„æµ‹
            if len(predictions) < num_predictions:
                rule_predictions = self._rule_based_prediction(input_line, num_predictions - len(predictions))
                predictions.extend(rule_predictions)
            
            # å»é‡å’Œè´¨é‡è¿‡æ»¤
            unique_predictions = []
            for pred in predictions:
                if pred not in unique_predictions and len(pred) > 2:
                    unique_predictions.append(pred)
            
            return unique_predictions[:num_predictions] if unique_predictions else ["æš‚æ— åˆé€‚çš„é¢„æµ‹ç»“æœ"]
            
        except Exception as e:
            return [f"é¢„æµ‹å‡ºé”™: {str(e)}"]
    
    def _model_based_prediction(self, input_line, num_predictions):
        """åŸºäºæ¨¡å‹çš„é¢„æµ‹"""
        words = self.preprocess_text(input_line)
        if not words:
            return []
        
        predictions = []
        
        for i in range(num_predictions):
            # ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å’Œéšæœºç§å­
            temperature = 0.8 + (i * 0.1)
            torch.manual_seed(42 + i * 7)
            
            input_indices = [self.vocab_to_idx.get(word, self.vocab_to_idx['<UNK>']) for word in words]
            
            if len(input_indices) > self.seq_length:
                input_indices = input_indices[-self.seq_length:]
            
            while len(input_indices) < self.seq_length:
                input_indices.insert(0, self.vocab_to_idx['<PAD>'])
            
            self.model.eval()
            with torch.no_grad():
                input_tensor = torch.tensor([input_indices])
                hidden = None
                predicted_words = []
                
                for step in range(15):  # æœ€å¤šç”Ÿæˆ15ä¸ªè¯
                    output, hidden = self.model(input_tensor, hidden)
                    logits = output[0, -1, :] / temperature
                    
                    # Top-ké‡‡æ ·
                    k = max(3, 8 - i)
                    top_k_logits, top_k_indices = torch.topk(logits, k)
                    top_k_probs = torch.softmax(top_k_logits, dim=0)
                    
                    selected_idx = torch.multinomial(top_k_probs, 1).item()
                    next_word_idx = top_k_indices[selected_idx].item()
                    next_word = self.idx_to_vocab[next_word_idx]
                    
                    if next_word in ['<END>', '<PAD>']:
                        break
                    
                    if next_word != '<UNK>' and (not predicted_words or next_word != predicted_words[-1]):
                        predicted_words.append(next_word)
                    
                    input_tensor = torch.cat([input_tensor[:, 1:], torch.tensor([[next_word_idx]])], dim=1)
                    
                    # æ—©åœæ¡ä»¶
                    if len(predicted_words) >= 6 and any(punct in ''.join(predicted_words) for punct in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
                        break
                
                result = ''.join(predicted_words)
                if result and len(result) > 2:
                    predictions.append(result)
        
        return predictions
    
    def _template_based_prediction(self, input_line, num_predictions):
        """åŸºäºæ¨¡æ¿çš„é¢„æµ‹"""
        predictions = []
        
        for i in range(num_predictions):
            template = random.choice(self.quality_templates)
            
            # éšæœºå¡«å……æ¨¡æ¿
            filled_template = template
            for category, words in self.word_categories.items():
                placeholder = f"{{{category}}}"
                if placeholder in filled_template:
                    replacement = random.choice(words)
                    filled_template = filled_template.replace(placeholder, replacement, 1)
            
            # æ¸…ç†æ¨¡æ¿ä¸­å‰©ä½™çš„å ä½ç¬¦
            filled_template = re.sub(r'\{[^}]+\}', '', filled_template)
            
            if filled_template and len(filled_template) > 2:
                predictions.append(filled_template)
        
        return predictions
    
    def _rule_based_prediction(self, input_line, num_predictions):
        """åŸºäºè§„åˆ™çš„é¢„æµ‹"""
        predictions = []
        
        # ç®€å•çš„è§„åˆ™ç”Ÿæˆ
        rule_patterns = [
            "å¦‚èŠ±èˆ¬ç¾ä¸½ç»½æ”¾",
            "åƒæ˜Ÿæ˜Ÿä¸€æ ·é—ªäº®",
            "æ¸©æš–å¦‚æ˜¥é£æ‹‚é¢",
            "æ¸…æ–°å¦‚æ™¨éœ²ç”˜ç”œ",
            "æ·±æƒ…å¦‚æµ·æ´‹æ— è¾¹",
            "çº¯çœŸå¦‚ç™½äº‘é£˜é€¸",
        ]
        
        for i in range(min(num_predictions, len(rule_patterns))):
            predictions.append(rule_patterns[i])
        
        return predictions
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\nğŸµ ä¼˜åŒ–ç‰ˆæ­Œè¯é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print("è¾“å…¥ä¸€å¥æ­Œè¯ï¼ŒAIå°†ä¸ºæ‚¨åˆ›ä½œä¸‹ä¸€å¥")
        print("è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("=" * 50)
        
        while True:
            user_input = input("\nğŸ¤ è¯·è¾“å…¥ä¸€å¥æ­Œè¯: ").strip()
            
            if user_input.lower() == 'quit':
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
                break
            
            if not user_input:
                print("è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                continue
            
            print(f"\nğŸ“ è¾“å…¥: {user_input}")
            predictions = self.predict_with_quality_control(user_input)
            
            print("ğŸ¼ AIåˆ›ä½œçš„ä¸‹ä¸€å¥:")
            for i, pred in enumerate(predictions, 1):
                print(f"   {i}. {pred}")
            print("-" * 50)


def main():
    """ä¸»å‡½æ•°"""
    predictor = OptimizedLyricsPredictor()
    
    print("ğŸµ ä¼˜åŒ–ç‰ˆæ­Œè¯é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 40)
    print("1. è®­ç»ƒæ–°çš„ä¼˜åŒ–æ¨¡å‹")
    print("2. åŠ è½½æ¨¡å‹å¹¶å¼€å§‹é¢„æµ‹")
    
    choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == '1':
        print("å¼€å§‹è®­ç»ƒä¼˜åŒ–æ¨¡å‹...")
        predictor.train_optimized_model(epochs=60)
        
        if predictor.load_model():
            predictor.interactive_mode()
    
    elif choice == '2':
        if predictor.load_model():
            predictor.interactive_mode()
        else:
            print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
    
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼")


if __name__ == "__main__":
    main() 