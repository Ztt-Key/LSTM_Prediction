import torch
import torch.nn as nn
import torch.optim as optim
import jieba
import numpy as np
import json
import os
import pickle
from collections import Counter
from torch.utils.data import Dataset, DataLoader
import re
from tqdm import tqdm
from lyrics_collector import LyricsCollector


class EnhancedLyricsPredictor:
    """å¢å¼ºç‰ˆæ­Œè¯é¢„æµ‹å™¨"""
    
    def __init__(self, model_path='enhanced_lyrics_model.pth', vocab_path='enhanced_vocab.pkl'):
        self.model_path = model_path
        self.vocab_path = vocab_path
        self.model = None
        self.vocab_to_idx = {}
        self.idx_to_vocab = {}
        self.seq_length = 15  # å¢åŠ åºåˆ—é•¿åº¦
        self.collector = LyricsCollector()
        
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰\[\]ã€ã€‘\s]', '', text)
        # ä½¿ç”¨jiebaåˆ†è¯
        words = list(jieba.cut(text.strip()))
        # è¿‡æ»¤ç©ºç™½è¯
        words = [word.strip() for word in words if word.strip()]
        return words
    
    def build_enhanced_vocab(self):
        """æ„å»ºå¢å¼ºè¯æ±‡è¡¨"""
        print("æ­£åœ¨æ„å»ºå¢å¼ºè¯æ±‡è¡¨...")
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®æ”¶é›†å™¨
        try:
            from enhanced_data_collector import EnhancedDataCollector
            enhanced_collector = EnhancedDataCollector()
            enhanced_data = enhanced_collector.collect_enhanced_data()
            print(f"ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®é›†ï¼Œå…± {len(enhanced_data)} æ¡æ•°æ®")
        except:
            print("å¢å¼ºç‰ˆæ•°æ®æ”¶é›†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®æ”¶é›†å™¨...")
            enhanced_data = self.collector.expand_lyrics_dataset()
        
        # é¢„å¤„ç†æ‰€æœ‰æ­Œè¯
        all_words = []
        processed_lyrics = []
        
        for lyric in enhanced_data:
            words = self.preprocess_text(lyric)
            if len(words) >= 3:  # è‡³å°‘3ä¸ªè¯æ‰æœ‰æ„ä¹‰
                all_words.extend(words)
                processed_lyrics.append(words)
        
        # ç»Ÿè®¡è¯é¢‘å¹¶æ„å»ºè¯æ±‡è¡¨
        word_counts = Counter(all_words)
        
        # æ·»åŠ ç‰¹æ®Štoken
        vocab = ['<PAD>', '<UNK>', '<START>', '<END>']
        # æ·»åŠ é«˜é¢‘è¯ï¼ˆå‡ºç°æ¬¡æ•°>=2çš„è¯ï¼‰
        frequent_words = [word for word, count in word_counts.most_common() if count >= 2]
        vocab.extend(frequent_words)
        
        self.vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx_to_vocab = {idx: word for word, idx in self.vocab_to_idx.items()}
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(self.vocab_path, 'wb') as f:
            pickle.dump((self.vocab_to_idx, self.idx_to_vocab), f)
        
        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå…±åŒ…å« {len(vocab)} ä¸ªè¯")
        return processed_lyrics
    
    def prepare_enhanced_training_data(self, lyrics_list):
        """å‡†å¤‡å¢å¼ºè®­ç»ƒæ•°æ®"""
        sequences = []
        
        for lyric in lyrics_list:
            # å¦‚æœlyricæ˜¯åˆ—è¡¨ï¼Œå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(lyric, list):
                lyric_text = ''.join(lyric)
            else:
                lyric_text = lyric
                
            words = self.preprocess_text(lyric_text)
            if len(words) < 3:  # è·³è¿‡å¤ªçŸ­çš„æ­Œè¯
                continue
                
            # æ·»åŠ å¼€å§‹å’Œç»“æŸtoken
            words = ['<START>'] + words + ['<END>']
            
            # åˆ›å»ºå¤šç§é•¿åº¦çš„åºåˆ—
            for seq_len in [self.seq_length - 2, self.seq_length, self.seq_length + 2]:
                for i in range(len(words) - seq_len):
                    sequence = words[i:i + seq_len + 1]
                    sequences.append(sequence)
        
        return sequences
    
    def train_enhanced_model(self, epochs=80, batch_size=16, learning_rate=0.001):
        """è®­ç»ƒå¢å¼ºæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒå¢å¼ºæ­Œè¯é¢„æµ‹æ¨¡å‹...")
        
        # æ„å»ºè¯æ±‡è¡¨å’Œè·å–æ•°æ®
        all_lyrics = self.build_enhanced_vocab()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        sequences = self.prepare_enhanced_training_data(all_lyrics)
        
        if not sequences:
            print("æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
            return
        
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab_to_idx)}")
        print(f"è®­ç»ƒåºåˆ—æ•°é‡: {len(sequences)}")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        from lyrics_predictor import LyricsDataset, LyricsLSTM
        
        dataset = LyricsDataset(sequences, self.vocab_to_idx, self.seq_length)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¢åŠ å¤æ‚åº¦ï¼‰
        vocab_size = len(self.vocab_to_idx)
        self.model = LyricsLSTM(
            vocab_size=vocab_size, 
            embedding_dim=256,  # å¢åŠ åµŒå…¥ç»´åº¦
            hidden_dim=512,     # å¢åŠ éšè—å±‚ç»´åº¦
            num_layers=3,       # å¢åŠ å±‚æ•°
            dropout=0.4
        )
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss(ignore_index=self.vocab_to_idx['<PAD>'])
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        best_loss = float('inf')
        
        for epoch in range(epochs):
            total_loss = 0
            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
            
            for batch_input, batch_target in progress_bar:
                optimizer.zero_grad()
                
                output, _ = self.model(batch_input)
                loss = criterion(output.reshape(-1, vocab_size), batch_target.reshape(-1))
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                total_loss += loss.item()
                progress_bar.set_postfix({'loss': loss.item()})
            
            scheduler.step()
            avg_loss = total_loss / len(dataloader)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                torch.save(self.model.state_dict(), self.model_path)
            
            if (epoch + 1) % 10 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.4f}')
        
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {self.model_path}")
    
    def load_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        try:
            # åŠ è½½è¯æ±‡è¡¨
            with open(self.vocab_path, 'rb') as f:
                self.vocab_to_idx, self.idx_to_vocab = pickle.load(f)
            
            # åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
            from lyrics_predictor import LyricsLSTM
            vocab_size = len(self.vocab_to_idx)
            self.model = LyricsLSTM(
                vocab_size=vocab_size, 
                embedding_dim=256,
                hidden_dim=512,
                num_layers=3,
                dropout=0.4
            )
            self.model.load_state_dict(torch.load(self.model_path, map_location='cpu'))
            self.model.eval()
            
            print("å¢å¼ºæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
        except FileNotFoundError:
            print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return False
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def predict_next_line_enhanced(self, input_line, max_length=25, temperature=0.7, num_predictions=3):
        """å¢å¼ºé¢„æµ‹åŠŸèƒ½ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰ç»“æœ"""
        if self.model is None:
            print("è¯·å…ˆåŠ è½½æ¨¡å‹")
            return ["æ¨¡å‹æœªåŠ è½½"]
        
        try:
            # é¢„å¤„ç†è¾“å…¥
            words = self.preprocess_text(input_line)
            if not words:
                return ["è¾“å…¥æ— æ•ˆï¼Œè¯·æä¾›æœ‰æ„ä¹‰çš„æ­Œè¯"]
            
            # è½¬æ¢ä¸ºç´¢å¼•
            input_indices = [self.vocab_to_idx.get(word, self.vocab_to_idx['<UNK>']) for word in words]
            
            # å¦‚æœè¾“å…¥å¤ªé•¿ï¼Œåªå–æœ€åå‡ ä¸ªè¯
            if len(input_indices) > self.seq_length:
                input_indices = input_indices[-self.seq_length:]
            
            # è¡¥å……åˆ°å›ºå®šé•¿åº¦
            while len(input_indices) < self.seq_length:
                input_indices.insert(0, self.vocab_to_idx['<PAD>'])
            
            predictions = []
            
            # ç”Ÿæˆå¤šä¸ªä¸åŒçš„é¢„æµ‹ç»“æœ
            for i in range(num_predictions):
                # ä¸ºæ¯æ¬¡é¢„æµ‹ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å’Œéšæœºç§å­
                current_temperature = temperature + (i * 0.1)  # é€’å¢æ¸©åº¦å¢åŠ å¤šæ ·æ€§
                if current_temperature > 1.5:
                    current_temperature = 0.5 + (i * 0.05)  # é¿å…è¿‡é«˜æ¸©åº¦
                
                # ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
                import torch
                torch.manual_seed(42 + i * 10)
                
                self.model.eval()
                with torch.no_grad():
                    input_tensor = torch.tensor([input_indices])
                    hidden = None
                    predicted_words = []
                    
                    # å¢åŠ éšæœºèµ·å§‹ç­–ç•¥
                    if i > 0:  # ç¬¬ä¸€ä¸ªé¢„æµ‹ä¿æŒåŸå§‹è¾“å…¥ï¼Œåç»­é¢„æµ‹æ·»åŠ éšæœºæ‰°åŠ¨
                        # éšæœºæ›¿æ¢æœ€åä¸€ä¸ªè¯ï¼ˆå¦‚æœä¸æ˜¯å…³é”®è¯ï¼‰
                        last_word_idx = input_indices[-1]
                        if last_word_idx not in [self.vocab_to_idx.get('<START>', 0), 
                                               self.vocab_to_idx.get('<END>', 1),
                                               self.vocab_to_idx.get('<PAD>', 2)]:
                            # ä»è¯æ±‡è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç›¸è¿‘çš„è¯
                            vocab_size = len(self.vocab_to_idx)
                            random_shift = torch.randint(-5, 6, (1,)).item()
                            new_idx = (last_word_idx + random_shift) % vocab_size
                            input_indices_copy = input_indices.copy()
                            input_indices_copy[-1] = new_idx
                            input_tensor = torch.tensor([input_indices_copy])
                    
                    for step in range(max_length):
                        output, hidden = self.model(input_tensor, hidden)
                        
                        # åº”ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥
                        logits = output[0, -1, :] / current_temperature
                        
                        # åŠ å…¥top-ké‡‡æ ·å¢åŠ å¤šæ ·æ€§
                        k = max(5, 10 - i)  # ä¸åŒé¢„æµ‹ä½¿ç”¨ä¸åŒçš„kå€¼
                        top_k_logits, top_k_indices = torch.topk(logits, k)
                        top_k_probs = torch.softmax(top_k_logits, dim=0)
                        
                        # ä»top-kä¸­éšæœºé‡‡æ ·
                        selected_idx = torch.multinomial(top_k_probs, 1).item()
                        next_word_idx = top_k_indices[selected_idx].item()
                        
                        next_word = self.idx_to_vocab[next_word_idx]
                        
                        # å¦‚æœé‡åˆ°ç»“æŸç¬¦æˆ–å¡«å……ç¬¦ï¼Œåœæ­¢ç”Ÿæˆ
                        if next_word in ['<END>', '<PAD>']:
                            break
                        
                        # è·³è¿‡æœªçŸ¥è¯å’Œé‡å¤è¯
                        if next_word != '<UNK>' and (not predicted_words or next_word != predicted_words[-1]):
                            predicted_words.append(next_word)
                        
                        # å¦‚æœè¿ç»­ç”Ÿæˆç›¸åŒçš„è¯ï¼Œå¼•å…¥éšæœºæ€§
                        if len(predicted_words) >= 2 and predicted_words[-1] == predicted_words[-2]:
                            # éšæœºè·³è¿‡è¿™ä¸ªè¯æˆ–é€‰æ‹©å¤‡é€‰è¯
                            if torch.rand(1).item() > 0.5:
                                predicted_words.pop()  # ç§»é™¤é‡å¤è¯
                                # é€‰æ‹©ç¬¬äºŒä¸ªæœ€å¯èƒ½çš„è¯
                                if k > 1:
                                    alt_idx = torch.multinomial(top_k_probs, 1).item()
                                    alt_word_idx = top_k_indices[alt_idx].item()
                                    alt_word = self.idx_to_vocab[alt_word_idx]
                                    if alt_word not in ['<UNK>', '<END>', '<PAD>']:
                                        predicted_words.append(alt_word)
                        
                        # æ›´æ–°è¾“å…¥åºåˆ—
                        input_tensor = torch.cat([input_tensor[:, 1:], torch.tensor([[next_word_idx]])], dim=1)
                        
                        # æå‰ç»“æŸæ¡ä»¶ï¼šå¦‚æœç”Ÿæˆäº†åˆç†é•¿åº¦çš„å¥å­
                        if step >= 8 and len(predicted_words) >= 5:
                            # æ£€æŸ¥æ˜¯å¦å½¢æˆäº†å®Œæ•´çš„æ„æ€
                            result_text = ''.join(predicted_words)
                            if any(punct in result_text for punct in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
                                break
                    
                    result = ''.join(predicted_words) if predicted_words else f"åˆ›æ„é¢„æµ‹{i+1}"
                    
                    # é¿å…å®Œå…¨ç›¸åŒçš„ç»“æœ
                    if result not in predictions:
                        predictions.append(result)
                    else:
                        # å¦‚æœç»“æœé‡å¤ï¼Œæ·»åŠ å˜åŒ–
                        modified_result = self._add_variation(result, i)
                        predictions.append(modified_result)
            
            return predictions if predictions else ["æ— æ³•ç”Ÿæˆé¢„æµ‹ï¼Œè¯·å°è¯•å…¶ä»–è¾“å…¥"]
            
        except Exception as e:
            print(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return [f"é¢„æµ‹å¤±è´¥: {str(e)}"]
    
    def _add_variation(self, text, variation_index):
        """ä¸ºé‡å¤çš„é¢„æµ‹ç»“æœæ·»åŠ å˜åŒ–"""
        try:
            words = list(jieba.cut(text))
            if len(words) < 2:
                return f"{text}ï¼ˆå˜åŒ–{variation_index + 1}ï¼‰"
            
            # ç®€å•çš„å˜åŒ–ç­–ç•¥ï¼šæ›¿æ¢æˆ–æ·»åŠ è¯æ±‡
            variation_words = ['ç¾å¥½', 'æ¸©æš–', 'æ¸…æ–°', 'åŠ¨äºº', 'ç”œç¾', 'æµªæ¼«', 'æ¢¦å¹»', 'çº¯çœŸ']
            transition_words = ['å¦‚', 'åƒ', 'ä¼¼', 'è‹¥', 'ä»¿ä½›', 'æ°ä¼¼']
            
            if variation_index % 2 == 0:
                # åœ¨ä¸­é—´æ·»åŠ ä¿®é¥°è¯
                mid_idx = len(words) // 2
                variation_word = variation_words[variation_index % len(variation_words)]
                words.insert(mid_idx, variation_word)
            else:
                # æ·»åŠ è¿‡æ¸¡è¯
                if len(words) >= 3:
                    transition_word = transition_words[variation_index % len(transition_words)]
                    words.insert(-2, transition_word)
            
            return ''.join(words)
        except:
            return f"{text}ï¼ˆç‰ˆæœ¬{variation_index + 1}ï¼‰"
    
    def interactive_prediction(self):
        """äº¤äº’å¼é¢„æµ‹ç•Œé¢"""
        print("\n=== å¢å¼ºæ­Œè¯é¢„æµ‹ç³»ç»Ÿ ===")
        print("è¾“å…¥ä¸€å¥æ­Œè¯ï¼Œç³»ç»Ÿå°†ä¸ºæ‚¨é¢„æµ‹å¯èƒ½çš„ä¸‹ä¸€å¥")
        print("è¾“å…¥ 'quit' é€€å‡ºç³»ç»Ÿ")
        print("-" * 50)
        
        while True:
            input_line = input("\nğŸµ è¯·è¾“å…¥ä¸€å¥æ­Œè¯: ").strip()
            
            if input_line.lower() == 'quit':
                print("æ„Ÿè°¢ä½¿ç”¨æ­Œè¯é¢„æµ‹ç³»ç»Ÿï¼")
                break
            
            if not input_line:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ­Œè¯å†…å®¹")
                continue
            
            print("\nğŸ¯ æ­£åœ¨é¢„æµ‹...")
            predictions = self.predict_next_line_enhanced(input_line)
            
            print(f"\nğŸ“ åŸºäºè¾“å…¥: \"{input_line}\"")
            print("ğŸ¼ é¢„æµ‹çš„å¯èƒ½ä¸‹ä¸€å¥:")
            
            for i, prediction in enumerate(predictions, 1):
                print(f"   {i}. {prediction}")
            
            print("-" * 50)


def main():
    """ä¸»å‡½æ•°"""
    predictor = EnhancedLyricsPredictor()
    
    print("=== å¢å¼ºæ­Œè¯é¢„æµ‹ç³»ç»Ÿ ===")
    print("1. è®­ç»ƒæ–°çš„å¢å¼ºæ¨¡å‹")
    print("2. åŠ è½½å·²æœ‰æ¨¡å‹å¹¶å¼€å§‹é¢„æµ‹")
    print("3. ä»…æ”¶é›†æ­Œè¯æ•°æ®")
    
    choice = input("è¯·é€‰æ‹©æ“ä½œ (1/2/3): ").strip()
    
    if choice == '1':
        print("å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹...")
        predictor.train_enhanced_model(epochs=60)
        
        # è®­ç»ƒå®Œæˆåè¿›å…¥äº¤äº’æ¨¡å¼
        if predictor.load_model():
            predictor.interactive_prediction()
    
    elif choice == '2':
        if predictor.load_model():
            predictor.interactive_prediction()
        else:
            print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
    
    elif choice == '3':
        collector = LyricsCollector()
        lyrics = collector.expand_lyrics_dataset()
        print(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(lyrics)} æ¡æ­Œè¯")
    
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼")


if __name__ == "__main__":
    main() 